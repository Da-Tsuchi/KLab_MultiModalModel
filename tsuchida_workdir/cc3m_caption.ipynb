{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kmmm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "from peft import LoraConfig, get_peft_model,PeftModel, PeftConfig\n",
    "sys.path.append(\"/home/tsuchida/KLab_MultiModalModel/tsuchida_workdir/..\")\n",
    "from PIL import Image\n",
    "from typing import List\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image,PngImagePlugin\n",
    "\n",
    "# Decompressed Data Too Largeになることを防ぐ\n",
    "LARGE_ENOUGH_NUMBER = 100\n",
    "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
    "\n",
    "class DatasetLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, resize=256):\n",
    "        self.images, self.tgt_texts, self.src_texts = [], [], []\n",
    "        self.src_transforms = transforms.Compose([\n",
    "            transforms.Resize((resize, resize)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.tgt_transforms = transforms.Compose([\n",
    "            transforms.Resize((resize, resize)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, src_text, tgt_text = self.images[idx], self.src_texts[idx], self.tgt_texts[idx]\n",
    "        image = Image.open(image).convert('RGB')\n",
    "        src_image = self.src_transforms(image)\n",
    "        tgt_image = self.src_transforms(image)\n",
    "\n",
    "        return src_image, tgt_image, src_text, tgt_text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class CC3MDatasetLoader(DatasetLoader):\n",
    "    def __init__(self,data_dir=\"/data01/cc3m\",phase=\"train\",imagesize=(256,256)):\n",
    "        super().__init__()\n",
    "        \n",
    "        with open(os.path.join(data_dir,f\"{phase}.tsv\"),\"r\") as f:\n",
    "            items = f.read()\n",
    "\n",
    "        items = items.split(\"\\n\")\n",
    "        items = [item.split(\"\\t\") for item in items]\n",
    "        num = int(len(items)/2)\n",
    "        # データセット半分のみ使用\n",
    "        items = items[1:num]\n",
    "\n",
    "        self.tgt_texts = [item[1] for item in items]\n",
    "        self.src_texts = [\"What does the image describe?\"]*len(items)\n",
    "        self.images = [os.path.join(data_dir,phase,item[0]) for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CC3MDatasetLoader(data_dir=\"/data01/cc3m\", phase=\"train\")\n",
    "val_dataset = CC3MDatasetLoader(data_dir=\"/data01/cc3m\", phase=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torchvision import transforms\n",
    "# src_tokenizer = AutoTokenizer.from_pretrained(args.language_model_name, model_max_length=256)\n",
    "src_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\", model_max_length=256, use_fast=True)\n",
    "\n",
    "# tgt_tokenizer = AutoTokenizer.from_pretrained(args.language_model_name, model_max_length=256, use_fast=True, extra_ids=0, additional_special_tokens =[f\"<extra_id_{i}>\" for i in range(100)] + [f\"<loc_{i}>\" for i in range(1000)] + [f\"<img_{i}>\" for i in range(args.image_vocab_size)])\n",
    "resize=256\n",
    "src_transforms = transforms.Compose([\n",
    "    transforms.Resize((resize, resize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "tgt_transforms = transforms.Compose([\n",
    "    transforms.Resize((resize, resize)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def custom_to_pil(x):\n",
    "  x = x.detach().cpu().numpy()\n",
    "  mean = np.array([0.485, 0.456, 0.406])\n",
    "  std = np.array([0.229, 0.224, 0.225])\n",
    "  x = (((x.transpose(1, 2, 0) * std) + mean) * 255.).astype(np.uint8)\n",
    "\n",
    "  x = Image.fromarray(x)\n",
    "  if not x.mode == \"RGB\":\n",
    "    x = x.convert(\"RGB\")\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 学習したものを可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = argparse.Namespace(\n",
    "    # Model setting\n",
    "    image_model_name=\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\",\n",
    "    image_model_train=False,\n",
    "    language_model_name=\"google/flan-t5-small\",\n",
    "    transformer_model_name=\"google/flan-t5-base\",\n",
    "    ffn=True,\n",
    "    phase = \"train\",\n",
    "    transformer_d_model=768,\n",
    "    transformer_d_ff=3072,\n",
    "    # transformer_d_model=1024,\n",
    "    # transformer_d_ff=4096,\n",
    "    transformer_d_kv=64,\n",
    "    transformer_num_heads=12,\n",
    "    transformer_num_layers=2,\n",
    "    transformer_num_decoder_layers=12,\n",
    "    image_vocab_size=16384,\n",
    "    loc_vocab_size=1600,\n",
    "    vae_ckpt_path=\"checkpoints/vqgan.pt\",\n",
    "    max_source_length=256,\n",
    "    max_target_length=256,\n",
    "    # Train setting\n",
    "    pretrain=\"train\", \n",
    "    # Dir setting\n",
    "    root_dir=\"/data01/\",\n",
    "    result_dir=\"results/\",\n",
    "    loss = \"CrossEntropy\",\n",
    "    loc_learn = \"lora\",\n",
    "    float_type = 'bfloat16',\n",
    "    lora_r = 4,\n",
    "    lora_alpha = 4,\n",
    "    lora_dropout = 0.1,\n",
    "    lora_bias = \"none\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_length\": 256,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 2,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "MyModel(\n",
      "  (language_model): T5EncoderModel(\n",
      "    (shared): Embedding(32128, 512)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 512)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 6)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-7): 7 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (image_model): Swinv2Model(\n",
      "    (embeddings): Swinv2Embeddings(\n",
      "      (patch_embeddings): Swinv2PatchEmbeddings(\n",
      "        (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): Swinv2Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=False)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.1)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.1)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0-17): 18 x Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.1)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (downsample): Swinv2PatchMerging(\n",
      "            (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): Swinv2Stage(\n",
      "          (blocks): ModuleList(\n",
      "            (0-1): 2 x Swinv2Layer(\n",
      "              (attention): Swinv2Attention(\n",
      "                (self): Swinv2SelfAttention(\n",
      "                  (continuous_position_bias_mlp): Sequential(\n",
      "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                    (1): ReLU(inplace=True)\n",
      "                    (2): Linear(in_features=512, out_features=48, bias=False)\n",
      "                  )\n",
      "                  (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                  (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "                  (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): Swinv2SelfOutput(\n",
      "                  (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (drop_path): Swinv2DropPath(p=0.1)\n",
      "              (intermediate): Swinv2Intermediate(\n",
      "                (dense): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): Swinv2Output(\n",
      "                (dense): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (transformer): T5ForConditionalGeneration(\n",
      "    (shared): Embedding(32128, 768)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 768)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 12)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (decoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32128, 768)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 12)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-11): 11 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseGatedActDense(\n",
      "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): NewGELUActivation()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      "  )\n",
      "  (language_ffn): Linear(in_features=512, out_features=768, bias=True)\n",
      "  (image_ffn): Linear(in_features=1536, out_features=768, bias=True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PeftModel(\n",
      "      (base_model): LoraModel(\n",
      "        (model): MyModel(\n",
      "          (language_model): T5EncoderModel(\n",
      "            (shared): Embedding(32128, 512)\n",
      "            (encoder): T5Stack(\n",
      "              (embed_tokens): Embedding(32128, 512)\n",
      "              (block): ModuleList(\n",
      "                (0): T5Block(\n",
      "                  (layer): ModuleList(\n",
      "                    (0): T5LayerSelfAttention(\n",
      "                      (SelfAttention): T5Attention(\n",
      "                        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                        (relative_attention_bias): Embedding(32, 6)\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (1): T5LayerFF(\n",
      "                      (DenseReluDense): T5DenseGatedActDense(\n",
      "                        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                        (dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (act): NewGELUActivation()\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (1-7): 7 x T5Block(\n",
      "                  (layer): ModuleList(\n",
      "                    (0): T5LayerSelfAttention(\n",
      "                      (SelfAttention): T5Attention(\n",
      "                        (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "                        (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                        (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "                        (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (1): T5LayerFF(\n",
      "                      (DenseReluDense): T5DenseGatedActDense(\n",
      "                        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                        (dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (act): NewGELUActivation()\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (final_layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (image_model): Swinv2Model(\n",
      "            (embeddings): Swinv2Embeddings(\n",
      "              (patch_embeddings): Swinv2PatchEmbeddings(\n",
      "                (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "              )\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (encoder): Swinv2Encoder(\n",
      "              (layers): ModuleList(\n",
      "                (0): Swinv2Stage(\n",
      "                  (blocks): ModuleList(\n",
      "                    (0-1): 2 x Swinv2Layer(\n",
      "                      (attention): Swinv2Attention(\n",
      "                        (self): Swinv2SelfAttention(\n",
      "                          (continuous_position_bias_mlp): Sequential(\n",
      "                            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                            (1): ReLU(inplace=True)\n",
      "                            (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "                          )\n",
      "                          (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                          (key): Linear(in_features=192, out_features=192, bias=False)\n",
      "                          (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                          (dropout): Dropout(p=0.0, inplace=False)\n",
      "                        )\n",
      "                        (output): Swinv2SelfOutput(\n",
      "                          (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                          (dropout): Dropout(p=0.0, inplace=False)\n",
      "                        )\n",
      "                      )\n",
      "                      (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                      (drop_path): Swinv2DropPath(p=0.1)\n",
      "                      (intermediate): Swinv2Intermediate(\n",
      "                        (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                        (intermediate_act_fn): GELUActivation()\n",
      "                      )\n",
      "                      (output): Swinv2Output(\n",
      "                        (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                        (dropout): Dropout(p=0.0, inplace=False)\n",
      "                      )\n",
      "                      (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (downsample): Swinv2PatchMerging(\n",
      "                    (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "                    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                  )\n",
      "                )\n",
      "                (1): Swinv2Stage(\n",
      "                  (blocks): ModuleList(\n",
      "                    (0-1): 2 x Swinv2Layer(\n",
      "                      (attention): Swinv2Attention(\n",
      "                        (self): Swinv2SelfAttention(\n",
      "                          (continuous_position_bias_mlp): Sequential(\n",
      "                            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                            (1): ReLU(inplace=True)\n",
      "                            (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "                          )\n",
      "                          (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                          (key): Linear(in_features=384, out_features=384, bias=False)\n",
      "                          (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                          (dropout): Dropout(p=0.0, inplace=False)\n",
      "                        )\n",
      "                        (output): Swinv2SelfOutput(\n",
      "                          (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                          (dropout): Dropout(p=0.0, inplace=False)\n",
      "                        )\n",
      "                      )\n",
      "                      (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                      (drop_path): Swinv2DropPath(p=0.1)\n",
      "                      (intermediate): Swinv2Intermediate(\n",
      "                        (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                        (intermediate_act_fn): GELUActivation()\n",
      "                      )\n",
      "                      (output): Swinv2Output(\n",
      "                        (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                        (dropout): Dropout(p=0.0, inplace=False)\n",
      "                      )\n",
      "                      (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (downsample): Swinv2PatchMerging(\n",
      "                    (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "                    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  )\n",
      "                )\n",
      "                (2): Swinv2Stage(\n",
      "                  (blocks): ModuleList(\n",
      "                    (0-17): 18 x Swinv2Layer(\n",
      "                      (attention): Swinv2Attention(\n",
      "                        (self): Swinv2SelfAttention(\n",
      "                          (continuous_position_bias_mlp): Sequential(\n",
      "                            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                            (1): ReLU(inplace=True)\n",
      "                            (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "                          )\n",
      "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                          (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                          (dropout): Dropout(p=0.0, inplace=False)\n",
      "                        )\n",
      "                        (output): Swinv2SelfOutput(\n",
      "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                          (dropout): Dropout(p=0.0, inplace=False)\n",
      "                        )\n",
      "                      )\n",
      "                      (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                      (drop_path): Swinv2DropPath(p=0.1)\n",
      "                      (intermediate): Swinv2Intermediate(\n",
      "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                        (intermediate_act_fn): GELUActivation()\n",
      "                      )\n",
      "                      (output): Swinv2Output(\n",
      "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                        (dropout): Dropout(p=0.0, inplace=False)\n",
      "                      )\n",
      "                      (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (downsample): Swinv2PatchMerging(\n",
      "                    (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "                    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "                  )\n",
      "                )\n",
      "                (3): Swinv2Stage(\n",
      "                  (blocks): ModuleList(\n",
      "                    (0-1): 2 x Swinv2Layer(\n",
      "                      (attention): Swinv2Attention(\n",
      "                        (self): Swinv2SelfAttention(\n",
      "                          (continuous_position_bias_mlp): Sequential(\n",
      "                            (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                            (1): ReLU(inplace=True)\n",
      "                            (2): Linear(in_features=512, out_features=48, bias=False)\n",
      "                          )\n",
      "                          (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                          (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "                          (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                          (dropout): Dropout(p=0.0, inplace=False)\n",
      "                        )\n",
      "                        (output): Swinv2SelfOutput(\n",
      "                          (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                          (dropout): Dropout(p=0.0, inplace=False)\n",
      "                        )\n",
      "                      )\n",
      "                      (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "                      (drop_path): Swinv2DropPath(p=0.1)\n",
      "                      (intermediate): Swinv2Intermediate(\n",
      "                        (dense): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                        (intermediate_act_fn): GELUActivation()\n",
      "                      )\n",
      "                      (output): Swinv2Output(\n",
      "                        (dense): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                        (dropout): Dropout(p=0.0, inplace=False)\n",
      "                      )\n",
      "                      (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "            (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "          )\n",
      "          (transformer): T5ForConditionalGeneration(\n",
      "            (shared): Embedding(33700, 768)\n",
      "            (encoder): T5Stack(\n",
      "              (embed_tokens): Embedding(33700, 768)\n",
      "              (block): ModuleList(\n",
      "                (0): T5Block(\n",
      "                  (layer): ModuleList(\n",
      "                    (0): T5LayerSelfAttention(\n",
      "                      (SelfAttention): T5Attention(\n",
      "                        (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                        (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                        (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                        (relative_attention_bias): Embedding(32, 12)\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (1): T5LayerFF(\n",
      "                      (DenseReluDense): T5DenseGatedActDense(\n",
      "                        (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                        (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                        (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                        (dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (act): NewGELUActivation()\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (1): T5Block(\n",
      "                  (layer): ModuleList(\n",
      "                    (0): T5LayerSelfAttention(\n",
      "                      (SelfAttention): T5Attention(\n",
      "                        (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                        (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                        (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (1): T5LayerFF(\n",
      "                      (DenseReluDense): T5DenseGatedActDense(\n",
      "                        (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                        (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                        (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                        (dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (act): NewGELUActivation()\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (final_layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (decoder): T5Stack(\n",
      "              (embed_tokens): Embedding(33700, 768)\n",
      "              (block): ModuleList(\n",
      "                (0): T5Block(\n",
      "                  (layer): ModuleList(\n",
      "                    (0): T5LayerSelfAttention(\n",
      "                      (SelfAttention): T5Attention(\n",
      "                        (q): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (k): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (v): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                        (relative_attention_bias): Embedding(32, 12)\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (1): T5LayerCrossAttention(\n",
      "                      (EncDecAttention): T5Attention(\n",
      "                        (q): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (k): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (v): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (2): T5LayerFF(\n",
      "                      (DenseReluDense): T5DenseGatedActDense(\n",
      "                        (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                        (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                        (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                        (dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (act): NewGELUActivation()\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (1-11): 11 x T5Block(\n",
      "                  (layer): ModuleList(\n",
      "                    (0): T5LayerSelfAttention(\n",
      "                      (SelfAttention): T5Attention(\n",
      "                        (q): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (k): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (v): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (1): T5LayerCrossAttention(\n",
      "                      (EncDecAttention): T5Attention(\n",
      "                        (q): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (k): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (v): Linear(\n",
      "                          in_features=768, out_features=768, bias=False\n",
      "                          (lora_dropout): ModuleDict(\n",
      "                            (default): Dropout(p=0.1, inplace=False)\n",
      "                          )\n",
      "                          (lora_A): ModuleDict(\n",
      "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
      "                          )\n",
      "                          (lora_B): ModuleDict(\n",
      "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
      "                          )\n",
      "                          (lora_embedding_A): ParameterDict()\n",
      "                          (lora_embedding_B): ParameterDict()\n",
      "                        )\n",
      "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (2): T5LayerFF(\n",
      "                      (DenseReluDense): T5DenseGatedActDense(\n",
      "                        (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                        (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "                        (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
      "                        (dropout): Dropout(p=0.1, inplace=False)\n",
      "                        (act): NewGELUActivation()\n",
      "                      )\n",
      "                      (layer_norm): T5LayerNorm()\n",
      "                      (dropout): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (final_layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lm_head): ModulesToSaveWrapper(\n",
      "              (original_module): Linear(in_features=768, out_features=33700, bias=False)\n",
      "              (modules_to_save): ModuleDict(\n",
      "                (default): Linear(in_features=768, out_features=33700, bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (language_ffn): ModulesToSaveWrapper(\n",
      "            (original_module): Linear(in_features=512, out_features=768, bias=True)\n",
      "            (modules_to_save): ModuleDict(\n",
      "              (default): Linear(in_features=512, out_features=768, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (image_ffn): ModulesToSaveWrapper(\n",
      "            (original_module): Linear(in_features=1536, out_features=768, bias=True)\n",
      "            (modules_to_save): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=768, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (criterion): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModel(\n",
       "      (base_model): LoraModel(\n",
       "        (model): MyModel(\n",
       "          (language_model): T5EncoderModel(\n",
       "            (shared): Embedding(32128, 512)\n",
       "            (encoder): T5Stack(\n",
       "              (embed_tokens): Embedding(32128, 512)\n",
       "              (block): ModuleList(\n",
       "                (0): T5Block(\n",
       "                  (layer): ModuleList(\n",
       "                    (0): T5LayerSelfAttention(\n",
       "                      (SelfAttention): T5Attention(\n",
       "                        (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                        (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                        (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                        (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                        (relative_attention_bias): Embedding(32, 6)\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (1): T5LayerFF(\n",
       "                      (DenseReluDense): T5DenseGatedActDense(\n",
       "                        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        (act): NewGELUActivation()\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (1-7): 7 x T5Block(\n",
       "                  (layer): ModuleList(\n",
       "                    (0): T5LayerSelfAttention(\n",
       "                      (SelfAttention): T5Attention(\n",
       "                        (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "                        (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "                        (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "                        (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (1): T5LayerFF(\n",
       "                      (DenseReluDense): T5DenseGatedActDense(\n",
       "                        (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                        (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "                        (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        (act): NewGELUActivation()\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (final_layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (image_model): Swinv2Model(\n",
       "            (embeddings): Swinv2Embeddings(\n",
       "              (patch_embeddings): Swinv2PatchEmbeddings(\n",
       "                (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "              )\n",
       "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (encoder): Swinv2Encoder(\n",
       "              (layers): ModuleList(\n",
       "                (0): Swinv2Stage(\n",
       "                  (blocks): ModuleList(\n",
       "                    (0-1): 2 x Swinv2Layer(\n",
       "                      (attention): Swinv2Attention(\n",
       "                        (self): Swinv2SelfAttention(\n",
       "                          (continuous_position_bias_mlp): Sequential(\n",
       "                            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                            (1): ReLU(inplace=True)\n",
       "                            (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "                          )\n",
       "                          (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                          (key): Linear(in_features=192, out_features=192, bias=False)\n",
       "                          (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                        (output): Swinv2SelfOutput(\n",
       "                          (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                      (drop_path): Swinv2DropPath(p=0.1)\n",
       "                      (intermediate): Swinv2Intermediate(\n",
       "                        (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): Swinv2Output(\n",
       "                        (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                    )\n",
       "                  )\n",
       "                  (downsample): Swinv2PatchMerging(\n",
       "                    (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "                (1): Swinv2Stage(\n",
       "                  (blocks): ModuleList(\n",
       "                    (0-1): 2 x Swinv2Layer(\n",
       "                      (attention): Swinv2Attention(\n",
       "                        (self): Swinv2SelfAttention(\n",
       "                          (continuous_position_bias_mlp): Sequential(\n",
       "                            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                            (1): ReLU(inplace=True)\n",
       "                            (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "                          )\n",
       "                          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "                          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                        (output): Swinv2SelfOutput(\n",
       "                          (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                      (drop_path): Swinv2DropPath(p=0.1)\n",
       "                      (intermediate): Swinv2Intermediate(\n",
       "                        (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): Swinv2Output(\n",
       "                        (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    )\n",
       "                  )\n",
       "                  (downsample): Swinv2PatchMerging(\n",
       "                    (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "                (2): Swinv2Stage(\n",
       "                  (blocks): ModuleList(\n",
       "                    (0-17): 18 x Swinv2Layer(\n",
       "                      (attention): Swinv2Attention(\n",
       "                        (self): Swinv2SelfAttention(\n",
       "                          (continuous_position_bias_mlp): Sequential(\n",
       "                            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                            (1): ReLU(inplace=True)\n",
       "                            (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                          )\n",
       "                          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "                          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                        (output): Swinv2SelfOutput(\n",
       "                          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                      (drop_path): Swinv2DropPath(p=0.1)\n",
       "                      (intermediate): Swinv2Intermediate(\n",
       "                        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): Swinv2Output(\n",
       "                        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    )\n",
       "                  )\n",
       "                  (downsample): Swinv2PatchMerging(\n",
       "                    (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "                    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                  )\n",
       "                )\n",
       "                (3): Swinv2Stage(\n",
       "                  (blocks): ModuleList(\n",
       "                    (0-1): 2 x Swinv2Layer(\n",
       "                      (attention): Swinv2Attention(\n",
       "                        (self): Swinv2SelfAttention(\n",
       "                          (continuous_position_bias_mlp): Sequential(\n",
       "                            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                            (1): ReLU(inplace=True)\n",
       "                            (2): Linear(in_features=512, out_features=48, bias=False)\n",
       "                          )\n",
       "                          (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                          (key): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                          (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                        (output): Swinv2SelfOutput(\n",
       "                          (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                          (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        )\n",
       "                      )\n",
       "                      (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                      (drop_path): Swinv2DropPath(p=0.1)\n",
       "                      (intermediate): Swinv2Intermediate(\n",
       "                        (dense): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "                        (intermediate_act_fn): GELUActivation()\n",
       "                      )\n",
       "                      (output): Swinv2Output(\n",
       "                        (dense): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "          )\n",
       "          (transformer): T5ForConditionalGeneration(\n",
       "            (shared): Embedding(33700, 768)\n",
       "            (encoder): T5Stack(\n",
       "              (embed_tokens): Embedding(33700, 768)\n",
       "              (block): ModuleList(\n",
       "                (0): T5Block(\n",
       "                  (layer): ModuleList(\n",
       "                    (0): T5LayerSelfAttention(\n",
       "                      (SelfAttention): T5Attention(\n",
       "                        (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                        (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                        (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                        (relative_attention_bias): Embedding(32, 12)\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (1): T5LayerFF(\n",
       "                      (DenseReluDense): T5DenseGatedActDense(\n",
       "                        (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                        (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                        (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        (act): NewGELUActivation()\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (1): T5Block(\n",
       "                  (layer): ModuleList(\n",
       "                    (0): T5LayerSelfAttention(\n",
       "                      (SelfAttention): T5Attention(\n",
       "                        (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                        (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                        (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (1): T5LayerFF(\n",
       "                      (DenseReluDense): T5DenseGatedActDense(\n",
       "                        (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                        (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                        (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        (act): NewGELUActivation()\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (final_layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (decoder): T5Stack(\n",
       "              (embed_tokens): Embedding(33700, 768)\n",
       "              (block): ModuleList(\n",
       "                (0): T5Block(\n",
       "                  (layer): ModuleList(\n",
       "                    (0): T5LayerSelfAttention(\n",
       "                      (SelfAttention): T5Attention(\n",
       "                        (q): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (k): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (v): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                        (relative_attention_bias): Embedding(32, 12)\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (1): T5LayerCrossAttention(\n",
       "                      (EncDecAttention): T5Attention(\n",
       "                        (q): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (k): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (v): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (2): T5LayerFF(\n",
       "                      (DenseReluDense): T5DenseGatedActDense(\n",
       "                        (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                        (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                        (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        (act): NewGELUActivation()\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (1-11): 11 x T5Block(\n",
       "                  (layer): ModuleList(\n",
       "                    (0): T5LayerSelfAttention(\n",
       "                      (SelfAttention): T5Attention(\n",
       "                        (q): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (k): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (v): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (1): T5LayerCrossAttention(\n",
       "                      (EncDecAttention): T5Attention(\n",
       "                        (q): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (k): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (v): Linear(\n",
       "                          in_features=768, out_features=768, bias=False\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Dropout(p=0.1, inplace=False)\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                        )\n",
       "                        (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (2): T5LayerFF(\n",
       "                      (DenseReluDense): T5DenseGatedActDense(\n",
       "                        (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                        (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                        (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                        (act): NewGELUActivation()\n",
       "                      )\n",
       "                      (layer_norm): T5LayerNorm()\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (final_layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lm_head): ModulesToSaveWrapper(\n",
       "              (original_module): Linear(in_features=768, out_features=33700, bias=False)\n",
       "              (modules_to_save): ModuleDict(\n",
       "                (default): Linear(in_features=768, out_features=33700, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (language_ffn): ModulesToSaveWrapper(\n",
       "            (original_module): Linear(in_features=512, out_features=768, bias=True)\n",
       "            (modules_to_save): ModuleDict(\n",
       "              (default): Linear(in_features=512, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (image_ffn): ModulesToSaveWrapper(\n",
       "            (original_module): Linear(in_features=1536, out_features=768, bias=True)\n",
       "            (modules_to_save): ModuleDict(\n",
       "              (default): Linear(in_features=1536, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (criterion): CrossEntropyLoss()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from models.model import MyModel\n",
    "model = MyModel(args).to(device)\n",
    "print(model)\n",
    "# path = \"/home/tsuchida/KLab_MultiModalModel/pth/caption/epoch_50.pth\"\n",
    "# model.load(result_name=path)\n",
    "tgt_tokenizer = AutoTokenizer.from_pretrained(args.language_model_name, model_max_length=args.max_target_length, use_fast=True, extra_ids=0, additional_special_tokens =[f\"<loc_{i}>\" for i in range(args.loc_vocab_size)])\n",
    "model.transformer.resize_token_embeddings(len(tgt_tokenizer))\n",
    "\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/lora/bf16/qkv/1e-5/openimage/1e-5lambda/enc2_dec12/epoch_40.pth\"\n",
    "# path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/bf16/scratch/openimage/enc2_dec12/epoch_40.pth\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/bf16/scratch/openimage/enc2_dec12/best.pth\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/bf16/lora/openimage/enc2_dec12/epoch_30.pth\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/bf16/lora/1e-4/openimage/enc2_dec12/bestLora\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/lora/bf16/scratch/1e-4/openimage/enc2_dec12/best.pth\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/bf16/scratch/1e-4/openimage/enc0_dec12/bestLora\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/bf16/scratch/1e-4/openimage/enc0_dec12/epoch_10.pth\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/loc/sample/openimage/1e-5lambda/enc0_dec12/bestLora\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/lora/vg/visual_genome_refexp/enc2_dec24/epoch_4\"\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/scratch/visual_genome_refexp/enc2_dec24/epoch_2.pth\"\n",
    "\n",
    "# train\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/scratch/base/visual_genome_refexp/enc2_dec12/epoch_50.pth\"\n",
    "# lora\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/lora/base/vg/visual_genome_refexp/enc2_dec12/epoch_50\"\n",
    "\n",
    "path = \"/home/tsuchida/KLab_MultiModalModel/results/1201/lora/2e-4/openimage_loc/enc2_dec12/epoch_50\"\n",
    "# path = \"/home/tsuchida/KLab_MultiModalModel/results/1207/openimage_loc/enc2_dec12/epoch_2\"\n",
    "\n",
    "\n",
    "if (args.loc_learn == \"lora\"):\n",
    "    model = PeftModel.from_pretrained(model, path)\n",
    "else:\n",
    "    model.load(result_name=path)\n",
    "# model = set_peft_model_state_dict(model, path)\n",
    "model = PeftModel.from_pretrained(model, path)\n",
    "print(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(boxA: List[float], boxB: List[float]) -> float:\n",
    "    # Compute the intersection area\n",
    "    interArea = max(0, min(boxA[2], boxB[2]) - max(boxA[0], boxB[0])) * max(0, min(boxA[3], boxB[3]) - max(boxA[1], boxB[1]))\n",
    "    if interArea == 0:\n",
    "        print(\"interArea: \", interArea)\n",
    "        return 0.0\n",
    "    # Compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    # Compute the intersection over union\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    print(\"iou: \", iou)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(dataset, idx=10):\n",
    "    src_image, tgt_image, src_text, tgt_text = dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        src_image = src_image.unsqueeze(0).to(device)\n",
    "        tgt_text2 = tgt_text\n",
    "        print('src_text:', src_text)\n",
    "        print('tgt_text:', tgt_text)\n",
    "        src_text = src_tokenizer(src_text, padding=\"longest\", max_length=args.max_source_length, return_tensors='pt')['input_ids'].to(device) # ['pt', 'tf', 'np', 'jax']\n",
    "        tgt_text = tgt_tokenizer(tgt_text, padding=\"longest\", max_length=args.max_target_length, return_tensors='pt')['input_ids'].to(device) # ['pt', 'tf', 'np', 'jax']\n",
    "        # print(src_text, tgt_text)\n",
    "\n",
    "        # display(custom_to_pil(src_image[0]))\n",
    "        src_attention_masks = torch.ones_like(src_text, device=device, dtype=torch.bool)\n",
    "        src_attention_masks[src_text == 0] = 0\n",
    "        tgt_attention_masks = torch.ones_like(tgt_text, device=device, dtype=torch.bool)\n",
    "        tgt_attention_masks[tgt_text == 0] = 0\n",
    "        preds= model(src_image, src_text, src_attention_masks, tgt_text, tgt_attention_masks,return_loss=False)\n",
    "\n",
    "        # print(loss)\n",
    "        preds = tgt_tokenizer.batch_decode(preds[:,1:-1])\n",
    "        print('pred:', preds[0])\n",
    "        print(type(preds))\n",
    "        img = custom_to_pil(src_image[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_text: What does the image describe?\n",
      "tgt_text: old cars by building function\n",
      "pred: eignen choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "show_result(train_dataset, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_text: What does the image describe?\n",
      "tgt_text: old cars by building function\n",
      "pred: eignen choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir choir\n",
      "<class 'list'>\n",
      "src_text: What does the image describe?\n",
      "tgt_text: sick business woman with flu blowing nose in tissue .\n",
      "pred: innovations choir Ari Poker Pokerjudicialjudicialjudicial choir 1966 Wool tablespoonjudicial choir ripple choir ripple choirjudicial choir 1966 impressive 1966lagen drum choir 1966lagen drum choir 1966 Wool choir 1966 bitfrunt 1966 Wool 1966 bit choir 1966 Wool 1966 bit choir 1966 bit choir bitfrunt bit choir bit choir bitfrunt choirjudicial choir 1966 Wool 1966 Wool Poker bit choir bit choir choir choir bitfrunt choir bit choir choir bitfrunt choir bit choir choir bit choir choir bitfrunt choir bitfrunt choir bit choir bit choir bit choir choir bitfrunt choir bitfrunt choir bit choir bit choir bit choir choir bit choir choir bit choir choir bitfrunt choir bit choir bit choir bit choir bit choir bitfrunt choir bit bit choir bit choir bit choir bit choir bit choir bit choir bit bit choir bit choir bit bit choir bit bit choir bit bit choir bit bit choir choir bit bit choir choir bit bit choir choir bit bit choir choir bit bitfrunt choir bit bit choir choir bit bit choir choir bit Ari bit choir bitfrunt choir bit bit choir choir bit Arijudicial choir bit Ari Poker bit bitfrunt choir bit choir choir bit Ari Pokerjudicial choir bit choir choir bit Ari tablespoon choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit Ari tablespoon choir bit choir choir bit choir choir bit choir choir bit bit choir choir\n",
      "<class 'list'>\n",
      "src_text: What does the image describe?\n",
      "tgt_text: view from the shore of a fjord during summer\n",
      "pred: sauber barbecue compréhension barbecue barbecue compréhension compréhension compréhension compréhension compréhensiongesehengesehenhoswinning educational barbecue barbecue barbecue choir choir compréhension barbecue barbecue barbecue choir choirgesehen educational Flooringfrunt barbecue barbecue barbecue barbecue choir 2007.94<extra_id_6>winning barbecuefrunt barbecue barbecue barbecue barbecue barbecue barbecue choirgesehen choir 2007. 2007. barbecue 2007.Ichwinning 2007. barbecue barbecue barbecue choir organizergesehen Pokerwinning 2007. barbecueIch 2007. chip barbecue barbecue barbecue choir 2007. 2007. chip barbecue choir choirgesehen Poker 2007. barbecue barbecue choirgesehen Poker 2007. barbecue barbecue choir organizer organizer organizer 2007. barbecue barbecue choirgesehen bitfrunt choir 2007. 2007. barbecue barbecue choir 2007. 2007. marathonwinning Flooringfrunt barbecue choir 2007. 2007. educational barbecuefrunt barbecue choir 2007. 2007.fruntServedwinningwinning 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007.gesehen 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007.gesehengesehengesehengesehen Ownfrunt barbecuefrunt barbecue1.8frunt barbecuehersteller barbecuehersteller<extra_id_50> 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. 2007. firms barbecue barbecue Brilliantfrunt searchfrunt barbecueenseignement barbecueenseignement barbecueenseignement barbecueenseignement barbecuealimentation chip chip chipphragesehen educational Volks barbecueenseignement barbecue chipphra 2007.KWwinninggesehen 2007.KW 2007. 2007.gesehen 2007.gesehen educational barbecueenseignement ist netfrunt barbecue chipfrunt choirgesehenfruntfrunt choirgesehenfrunt choir 2007. educational barbecue barbecue satisfied\n",
      "<class 'list'>\n",
      "src_text: What does the image describe?\n",
      "tgt_text: actors attend the world premiere\n",
      "pred: LICeignen Poker Pokereigneneignen2/faulty rotation barbecue compact engineeringfrunteignen rotation barbecuefrunt barbecuefrunt barbecuefruntfrunt barbecue necesităogenfrunteignen barbecue choirfrunt barbecuefruntfruntfruntfruntfrunt barbecuefruntfrunt barbecuefruntfruntfruntfruntfruntfruntfruntfruntfruntfrunt barbecuefrunt barbecue barbecuefruntfrunt barbecuefruntfruntfruntfruntfruntfruntfruntfruntfruntfrunt subiectfrunt hervor barbecue barbecuefruntjackjackjackjack facultyfrunt barbecue choirfruntfrunt hervor Hot Pokerjack facultyfruntfrunt hervor L Thur tablespoonfrunt hervor L Thur $35 compartimentcapturingÎ Poker America blackjack America blackjackÎ Poker America fixtures blackjack AmericaÎ Poker America America America America America America Honda cosmetic blackjack Honda cosmetic 1966 America America America compactfrunt hervor fixtures America bitfrunt hervor fixtures Cont conseiller astronaut blackjack AmericaÎ Poker Americaeignen barbecue1.81291.8 America fixtures Hot tablespoon rezolv barbecueuring blackjack America choir Cont tablespoonfruntfrunt hervor L laundry AmericaÎ femme Boston America Hondafruntfrunt hervor L Thur L Thur L laundry sauber inkl blackjack AmericaÎ hesitation inklgesehen blackjack America inkl loculdistilled blackjack America inkl locul weavingjudicial discipline America Hot tablespoon Conteignen compact compactcapturing1.8 barbecue1.8capturing weaving grains Conteignen Constanta inkl HotÎ compactfrunt hervor fixtures ConteignenÎ hesitationeignen1.8 compactcapturinguring Conteignen novels America AmericaÎ Hoteignen inkl Hot qualities cosmetic AmericaÎ femme indépendant choir\n",
      "<class 'list'>\n",
      "src_text: What does the image describe?\n",
      "tgt_text: pop artist poses for a portrait during awards\n",
      "pred: Served18 Off Pfarr educational Pfarr educationalcertifiedcertifiedcertifiedcertified faculty 52capturing Pfarrfruntcertifiedgesehen facultycertifiedgesehengesehengesehengesehen educational 52liga Ownfrunt educationalcapturinggesehen choir 52liga nervousfrunt choir bitfrunt choir nervous 2007. Publikum educationaléconomiecertifiedgesehen choir choir choir choir choir Pfarr choir Pfarr choir bit moments choir choir choir choir Pfarr choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir Pfarr choir choir bit choir choir bit Pfarr choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir Pfarr choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir Pfarr choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir Pfarr choir choir bit choir choir bit choir choir bit choir choir Pfarr choir choir bit choir choir bit choir choir bit Pfarr choir Pfarr choir choir bit choir choir bit choir choir bit choir Pfarr choir bit choir choir bit choir choir bit choir Pfarr choir bit choir choir bit choir choir bit choir choir bit choir Pfarr choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir Pfarr choir bit choir choir bit choir choir bit choir choir bit choir choir bit choir choir\n",
      "<class 'list'>\n",
      "src_text: What does the image describe?\n",
      "tgt_text: author : a life in photography -- in pictures\n",
      "pred: sauber sanitation barbecue erhöhtsemn barbecue vizitatori multinational vizitatori vizitatori choir creşte satisfiedgesehenuringgefüllt multinational barbecue vizitatori vizitatori vizitatori vizitatori conseiller barbecue barbecue Flickr choir conseillergefülltgesehen necesită verfolgt sanitation vizitatori barbecuegefüllt conseillergefüllt conseillergefülltgefülltgefülltgefüllt barbecue barbecue barbecue barbecue fonctionnalité sanitation barbecue Flickr choirgesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehen conseiller hervor barbecue barbecue barbecue choir conseiller barbecue vizitatori choir conseillergefüllt barbecue vizitatori choir tablespoon educational barbecue vizitatori choir conseiller barbecue compound choir conseillergefülltgefülltgefülltgefüllt Own erhöht educationalgefüllt educationalgefüllt barbecue vizitatori choir tablespoon choir conseiller barbecue choir conseiller barbecue choir conseillergefüllt barbecue choir tablespoon choir conseillergefüllt barbecue choir tablespoon choir conseillergefüllt conseiller hervor barbecue vizitatori choir choir tablespoonpie choir vizitatori choir choir Layer choir vizitatori choir vizitatori choir conseiller barbecue choir conseiller barbecue choir conseillergefüllt panels Layer choir conseillergefüllt panels Charge Ukrainian choir compactcapturing choir vizitatori choir vizitatori choirpie choirgesehen conseiller barbecue choir conseillergefüllt panelscapturing choir choir conseillergefüllt panels Poker erhöht choir vizitatori choir vizitatori choirpie choir conseiller barbecue choir conseillergefüllt panels Poker erhöht choir choir conseillergefüllt panels Poker hervor\n",
      "<class 'list'>\n",
      "src_text: What does the image describe?\n",
      "tgt_text: sandcastle beach on bright sky .\n",
      "pred: saubergesehen Own vizitatorigesehen Own vizitatori educational Woolcapturing sauber sanitation blackjackcapturingfruntgesehengesehen conseillergesehengesehengesehengesehengesehen educational educationalgesehen conseiller novelsgesehengesehen conseiller novels novels novels novels novelsgesehengesehengesehen conseillergesehengesehengesehengesehengesehengesehen conseillergesehengesehengesehengesehengesehengesehen conseiller novelsgesehen educationalfrunt educationalfrunt educationalfrunt educational educational 2007.capturinggesehen educationalfrunt educationalfrunt conseiller novels novels novels novels novels novels novelsgesehen educational educational educationalfrunt conseiller novelsgesehengesehen conseiller novelsgesehengesehen conseillergesehengesehen conseillergesehengesehengesehen conseillergesehengesehengesehengesehengesehen conseillergesehengesehengesehengesehengesehen conseiller novelsgesehen educationalfrunt educationalfrunt educationalfrunt conseiller novels novels novels novels novels novels novelsgesehen educationalfrunt educationalfrunt conseiller novels novels novels novels novels novels novelsgesehen educationalcapturing 2007.capturing 2007.capturinggesehen conseiller novels novels novels novels novels novels novels novels novels novels novelsgesehengesehen conseiller novelsgesehen conseiller novelsgesehengesehen conseiller novelsgesehen conseillergesehen conseillergesehen conseillergesehen conseillergesehen conseillergesehen conseillergesehen conseillergesehen conseillergesehengesehen conseillergesehengesehen conseillergesehengesehen conseiller novelsgesehengesehen conseiller novelsgesehengesehen conseiller novelsgesehengesehen conseiller novels novels novels novels novels novels novelsgesehengesehen conseiller novelsgesehengesehen conseiller novelsgesehengesehen conseiller novelsgesehengesehen conseiller novels novels novels novels novels novels novels novelsgesehengesehengesehen conseiller novelsgesehengesehen conseiller novelsgesehengesehen conseillergesehengesehen conseiller\n",
      "<class 'list'>\n",
      "src_text: What does the image describe?\n",
      "tgt_text: cruise ship and boats by the dock\n",
      "pred: Servedgesehenopathie Manufacturing tablespoon1.8capturingpolitikenseignement freshlycapturing 2007.capturing 2007. chip choirenseignement chip choir 2007. chip choir choir Pfarr educationalenseignementenseignementenseignement fonctionnalitécertifiedenseignementjackgesehengesehengesehengesehen educational educational 2007. compact chip choirenseignement educationalgesehen Manufacturing blame Manufacturing choir choirgesehen educational 2007.frunt educational 2007.jackgesehen educational 2007.jack Manufacturingcapturinggesehen educationalcapturing Manufacturing Manufacturing tablespoon educationalIch Ownffecapturing educational 2007.capturinggesehen educational Pokerenseignementjackgesehen educationalgesehen choirgesehengesehencapturing Anforderungenfrunt choir Manufacturingfrunt choir 2007. chip choirgesehen educational educational Manufacturing automated 2007. compact choir Manufacturingcapturing weaving Manufacturingcapturing weavinggesehen educationalfoughtgesehen choir Manufacturingcapturinggesehen choir ManufacturingcapturingServedgesehencapturinggesehen choirgesehengesehen choir Manufacturinggesehen choir Manufacturing Manufacturingcapturinggesehen choir Manufacturing Manufacturing Poker Poker fixturesgesehen choir educationalcapturinggesehen choir Manufacturingfrunt Manufacturing choir Manufacturingcapturing Manufacturingfruntgesehen choir ist Own 2007.hersteller fixtures Manufacturing fixturesfrem tablespoonfruntfrunt Manufacturing dezvolt choir Manufacturingfruntgesehen choir Manufacturingfrunt Manufacturing tablespoon 2007.capturing 2007.capturinggesehen choir Manufacturingcapturinggesehen choir novels educationalfought Manufacturing tile choir novels choirfrunt Own 2007.herstellergesehen choir Manufacturingcapturing educationalfought Manufacturingcapturinggesehen choir novels dezvolt choir choir choir choircapturinggesehen choirgesehen choir Manufacturingcapturing educationalfought fixtures educationalfoughtServedgesehengesehen choir novels choir choir choir choir choir compactcapturing educationalfought hesitation Manufacturing fixtures educationalfought fell choir choir\n",
      "<class 'list'>\n",
      "src_text: What does the image describe?\n",
      "tgt_text: the - bedroom stone cottage can sleep people\n",
      "pred: attended<extra_id_6><extra_id_6><extra_id_6> Woolcapturing compartimentwinninggesehenautoimmune sanitation sanitation optimized educational<extra_id_6>winning18<extra_id_6>Î conseillerigueurigueur educational conseiller 2007.<extra_id_6>containingcontaining conseiller 2007. Poker Bulgaria<extra_id_6>igueurigueurigueur gagnantgesehen educational finding găsi Reserve panels Anschluss rotationgesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehen designated RO BBQ sanitationgesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehengesehen designatedgefüllt conseillergesehengesehen designated barbecue thinking designated contour Hockeypolitik conseiller conseiller thinking Poker designated choir Poker Bulgaria bit Poker Poker Bulgaria biteignen Pokereignen conseiller finding conseillergefüllt conseiller priorities Anschluss educational<extra_id_6>leitet conseiller thinking finding consideration Flooring treasure sanitation<extra_id_6>leitet conseillergefüllt conseiller thinking finding choir compartimentcapturing consideration<extra_id_6> Poker Poker treasure Hockey Bulgaria bit Pokereignen Poker Poker Poker Poker Poker Poker Poker bit Poker biteignen Poker bit Poker bit Poker bit Poker biteignen Poker bit Poker bit Poker biteignen Pokereignen conseillergefüllt conseiller thinking finding conseillergefüllt conseiller thinking finding conseillergefüllt conseiller thinking finding conseillergefüllt conseiller thinking findinggesehen conseillergefüllt conseiller thinking finding conseillergefüllt conseiller thinking Pokereignen panelsleitet conseiller thinking Poker Pokereignen finding conseillergefüllt conseiller thinking Poker Pokereignen finding conseillergefüllt conseiller Bulgaria bit\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "show_result(train_dataset, idx=0)\n",
    "show_result(train_dataset, idx=1)\n",
    "show_result(train_dataset, idx=2)\n",
    "show_result(train_dataset, idx=6)\n",
    "show_result(train_dataset, idx=7)\n",
    "show_result(val_dataset, idx=0)\n",
    "show_result(val_dataset, idx=1)\n",
    "show_result(val_dataset, idx=5)\n",
    "show_result(val_dataset, idx=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30300, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenizer.encode('Whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kmmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
